{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5: Identify Fraud from Enron Email\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Goal of the Project \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal of this project is to build a predictive model using the enron financial dataset and the email dataset which can predict Person of Interest who are not already marked as a Person of Interest . Here I will try to select the most import features that will helps us predict the Enron Person of Interest. <br>\n",
    "    \n",
    "**The Dataset**<br>\n",
    "\n",
    "The dataset have information of *146 rows* each having *21 fields* . There are already *18 Person of Interest* marked in the Dataset . I will use the features of those 18 POIs to predict other POIs \n",
    "<br>\n",
    "\n",
    "**Sample Data Point**<br>\n",
    "\n",
    "    {'bonus': 600000,\n",
    "     'deferral_payments': 'NaN',\n",
    "     'deferred_income': 'NaN',\n",
    "     'director_fees': 'NaN',\n",
    "     'email_address': 'mark.metts@enron.com',\n",
    "     'exercised_stock_options': 'NaN',\n",
    "     'expenses': 94299,\n",
    "     'from_messages': 29,\n",
    "     'from_poi_to_this_person': 38,\n",
    "     'from_this_person_to_poi': 1,\n",
    "     'loan_advances': 'NaN',\n",
    "     'long_term_incentive': 'NaN',\n",
    "     'other': 1740,\n",
    "     'poi': False,\n",
    "     'restricted_stock': 585062,\n",
    "     'restricted_stock_deferred': 'NaN',\n",
    "     'salary': 365788,\n",
    "     'shared_receipt_with_poi': 702,\n",
    "     'to_messages': 807,\n",
    "     'total_payments': 1061827,\n",
    "     'total_stock_value': 585062}\n",
    " <br>\n",
    " \n",
    "**Outliers**<br>\n",
    "<br>\n",
    "<img src=\"plots/p1.png\"/><br>\n",
    "After I plot a scatterplot using 'salary' and 'bonus' I easily detect the outlier with key 'TOTAL' which stores the total value of the all the data points . So I just delete the 'TOTAL' data point from our dataset.<br>\n",
    "<img src=\"plots/p2.png\"/>\n",
    "And after I deleted the 'TOTAL' I still saw some data_points with realatively high values when I inspect the dataset and found those points represent the big bosses of Enron and they are some of the POIs . So we keep those outliers ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Feature Selection and Scaling\n",
    "\n",
    "In our case every data point have 21 fields and for missing values it is marked as 'NaN' . Every field is a potential feature but we will inspect the data through various visualizations <br>\n",
    "**Adding New Features**<br>\n",
    "We add two new fields to each of our datapoints. **fraction_to_messages** and **fraction_from_messages** .\n",
    "    \n",
    "    fraction_to_messages=from_this_person_to_poi/from_messages\n",
    "    fraction_from_messages=from_poi_to_this_person/to_messages\n",
    "    \n",
    " <img src=\"plots/p3.png\"/><br>\n",
    "These to fields store the fraction of the emails send to poi and received from poi.<br>\n",
    "\n",
    "**Selecting Best Five Features**<br>\n",
    "There are 20 + 2 fields now, but some of the fields don't have any effect on whether the person is a Person of Interest or not .Now I chose 13 features from the 22 and pass them to *sklearn* **SelectKBest()** where **f_classif** is the scoring algorithm.<br>\n",
    "\n",
    "    'poi'                     'salary'                   'bonus'        \n",
    "    'deferred_income'         'director_fees'            'exercised_stock_options'  \n",
    "    'expenses'                'fraction_from_poi'        'fraction_to_poi'\n",
    "    'loan_advances'           'long_term_incentive'      'restricted_stock'\n",
    "    'total_payments'          'total_stock_value'\n",
    "\n",
    "*Scores of Each Feature*<br>\n",
    "\n",
    "<br> <img src=\"plots/p4.png\"/><br>\n",
    "\n",
    "\n",
    "*The Top Five Features*\n",
    "\n",
    "    Feature Name                  Score\n",
    "    ------------                  -----\n",
    "    exercised_stock_options       25.0975415287\n",
    "    total_stock_value             24.4676540475\n",
    "    bonus                         21.0600017075\n",
    "    salary                        18.575703268\n",
    "    fraction_to_poi               16.6417070705\n",
    "    \n",
    "    \n",
    "**Feature Scaling**<br>\n",
    "Now I used to Sklearn *MinMaxScaler()* to scale my best five features in the range(0,1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.Algorithm Selection\n",
    "\n",
    "        GaussianNB\n",
    "        LinearSVC\n",
    "        DecisionTree\n",
    "        AdaBoost\n",
    "\n",
    "****\n",
    "\n",
    " **GaussianNB Performance**\n",
    "\n",
    "    Precision  0.5\n",
    "    Recall  0.6\n",
    "    F1_score  0.545454545455\n",
    "\n",
    "**Dtree Performance**\n",
    "\n",
    "    Precision  0.333333333333\n",
    "    Recall  0.4\n",
    "    F1_score  0.363636363636\n",
    "    \n",
    "**LinearSVC Performance**\n",
    "\n",
    "    Precision  0.0\n",
    "    Recall  0.0\n",
    "    F1_score  0.0\n",
    "    \n",
    "    \n",
    "Here we can See **GaussianNB** performs better than **Decision Tree**. So we will use **GaussianNB** for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  4.Algorithm Parameter Tuning\n",
    "\n",
    "Using Sklearn **GridSearchCV** I found the best parameters for Decision Tree are\n",
    "****\n",
    "\n",
    "**Decision Tree Performance**\n",
    "\n",
    "    Precision  0.4\n",
    "    Recall  0.4\n",
    "    F1_score  0.4\n",
    "\n",
    "    \n",
    "**Best Parameters**\n",
    "\n",
    "    min_samples_split: 3\n",
    "    criterion: 'entropy'\n",
    "    min_samples_leaf: 8\n",
    "    class_weight=None\n",
    "    max_depth=None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.Validation\n",
    "Validation is the process of testing a model with known inputs and test various factors on how well the model can identify unknown inputs.<br>\n",
    "The common mistake one can do while validating is using the same data for Training and Testing purpose.<br>\n",
    "Here we split the total dataset with 30% in the test set and 70% in the training Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Evaluation Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
