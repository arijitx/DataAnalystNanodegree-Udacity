{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5: Identify Fraud from Enron Email\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Goal of the Project \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal of this project is to build a predictive model using the enron financial dataset and the email dataset which can predict Person of Interest who are not already marked as a Person of Interest . Here I will try to select the most import features that will helps us predict the Enron Person of Interest. <br>\n",
    "    \n",
    "**The Dataset**<br>\n",
    "\n",
    "The dataset have information of *146 rows* each having *21 fields* . There are already *18 Person of Interest* marked in the Dataset . I will use the features of those 18 POIs to predict other POIs \n",
    "<br>\n",
    "\n",
    "**Sample Data Point**<br>\n",
    "\n",
    "    {'bonus': 600000,\n",
    "     'deferral_payments': 'NaN',\n",
    "     'deferred_income': 'NaN',\n",
    "     'director_fees': 'NaN',\n",
    "     'email_address': 'mark.metts@enron.com',\n",
    "     'exercised_stock_options': 'NaN',\n",
    "     'expenses': 94299,\n",
    "     'from_messages': 29,\n",
    "     'from_poi_to_this_person': 38,\n",
    "     'from_this_person_to_poi': 1,\n",
    "     'loan_advances': 'NaN',\n",
    "     'long_term_incentive': 'NaN',\n",
    "     'other': 1740,\n",
    "     'poi': False,\n",
    "     'restricted_stock': 585062,\n",
    "     'restricted_stock_deferred': 'NaN',\n",
    "     'salary': 365788,\n",
    "     'shared_receipt_with_poi': 702,\n",
    "     'to_messages': 807,\n",
    "     'total_payments': 1061827,\n",
    "     'total_stock_value': 585062}\n",
    " <br>\n",
    " \n",
    "**Missing Values**<br>\n",
    "We can see for the sample datapoint some the fields are marked as 'NaN' which implies we don't have any valid data for that field . Here to remove 'NaN' values from the final training set we will replace the 'NaN' with 0.0 and if for a particular row all the selected features have 'NaN' values then we have to discard that row for training .\n",
    "**Outliers**<br>\n",
    "<br>\n",
    "<img src=\"plots/p1.png\"/><br>\n",
    "After I plot a scatterplot using 'salary' and 'bonus' I easily detect the outlier with key 'TOTAL' which stores the total value of the all the data points . So I just delete the 'TOTAL' data point from our dataset.<br>\n",
    "<img src=\"plots/p2.png\"/>\n",
    "And after I deleted the 'TOTAL' I still saw some data_points with realatively high values when I inspect the dataset and found those points represent the big bosses of Enron and they are some of the POIs . So we keep those outliers ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Feature Selection and Scaling\n",
    "\n",
    "In our case every data point have 21 fields and for missing values it is marked as 'NaN' . Every field is a potential feature but we will inspect the data through various visualizations <br>\n",
    "**Adding New Features**<br>\n",
    "We add two new fields to each of our datapoints. **fraction_to_messages** and **fraction_from_messages** .\n",
    "    \n",
    "    fraction_to_messages=from_this_person_to_poi/from_messages\n",
    "    fraction_from_messages=from_poi_to_this_person/to_messages\n",
    "    \n",
    " <img src=\"plots/p3.png\"/><br>\n",
    "These to fields store the fraction of the emails send to poi and received from poi.<br>\n",
    "Here I used these two features `fraction_to_messages` and `fraction_from_messages` because these two features basically represent the 4 features `from_messages` , `from_poi_to_this_person` , `from_this_person_to_poi` , `to_messages` in a more scaled way . <br>\n",
    "**Selecting Best Five Features**<br>\n",
    "There are 20 + 2 fields now, but some of the fields don't have any effect on whether the person is a Person of Interest or not .Now I chose 17 features from the 22 and pass them to *sklearn* **SelectKBest()** where **f_classif** is the scoring algorithm.<br>\n",
    "Here I excluded 4 features `from_messages` , `from_poi_to_this_person` , `from_this_person_to_poi` , `to_messages` because I use `fraction_to_messages` and `fraction_from_messages` which represent the same 4 features . And I removed `email_address` .<br>\n",
    "\n",
    "\n",
    "    'poi'                       'salary'                   'bonus'        \n",
    "    'deferred_income'           'director_fees'            'exercised_stock_options'  \n",
    "    'expenses'                  'fraction_from_poi'        'fraction_to_poi'\n",
    "    'loan_advances'             'long_term_incentive'      'restricted_stock'\n",
    "    'total_payments'            'total_stock_value'        'deferral_payments'\n",
    "    'restricted_stock_deferred' 'shared_receipt_with_poi'\n",
    "\n",
    "*Scores of Each Feature*<br>\n",
    "\n",
    "\n",
    " \n",
    "<br> <img src=\"plots/p4.png\"/><br>\n",
    "Now we select the features with score at least 15 .<br>\n",
    "\n",
    "*The Top Five Features*\n",
    "\n",
    "    Feature Name                  Score\n",
    "    ------------                  -----\n",
    "    exercised_stock_options       25.0975415287\n",
    "    total_stock_value             24.4676540475\n",
    "    bonus                         21.0600017075\n",
    "    salary                        18.575703268\n",
    "    fraction_to_poi               16.6417070705\n",
    "    \n",
    "    \n",
    "**Feature Scaling**<br>\n",
    "Now I used to Sklearn *MinMaxScaler()* to scale my best five features in the range(0,1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.Algorithm Selection\n",
    "\n",
    "        GaussianNB\n",
    "        LinearSVC\n",
    "        DecisionTree\n",
    "        AdaBoost\n",
    "\n",
    "****\n",
    "\n",
    " **GaussianNB Performance**\n",
    "\n",
    "    Precision  0.5\n",
    "    Recall  0.6\n",
    "    F1_score  0.545454545455\n",
    "\n",
    "**Dtree Performance**\n",
    "\n",
    "    Precision  0.333333333333\n",
    "    Recall  0.4\n",
    "    F1_score  0.363636363636\n",
    "    \n",
    "**LinearSVC Performance**\n",
    "\n",
    "    Precision  0.0\n",
    "    Recall  0.0\n",
    "    F1_score  0.0\n",
    "    \n",
    "    \n",
    "Here we can See **GaussianNB** performs better than **Decision Tree**. So we will use **GaussianNB** for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  4.Algorithm Parameter Tuning\n",
    " \n",
    "Tuning a machine learning algorithm is important because it is not possible to estimate the parameters for a particular model for optimum performance. Here we use F1_score to detect the most optimum parameters for our model.<br>\n",
    "\n",
    "Using Sklearn **GridSearchCV** I found the best parameters for Decision Tree are\n",
    "****\n",
    "\n",
    "**Decision Tree Performance**\n",
    "\n",
    "    Precision  0.4\n",
    "    Recall  0.4\n",
    "    F1_score  0.4\n",
    "\n",
    "    \n",
    "**Best Parameters**\n",
    "\n",
    "    min_samples_split: 3\n",
    "    criterion: 'entropy'\n",
    "    min_samples_leaf: 8\n",
    "    class_weight=None\n",
    "    max_depth=None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.Validation\n",
    "Validation is the process of testing a model with known inputs and test various factors on how well the model can identify unknown inputs.<br>\n",
    "The common mistake one can do while validating is using the same data for Training and Testing purpose.<br>\n",
    "Here we split the total dataset with 30% in the test set and 70% in the training Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Evaluation Metrics\n",
    "\n",
    "Here we use the  **StratifiedShuffleSplit** to shuffle our dataset for 1000 folds . We find 2 Metrics about our model . <br>\n",
    "\n",
    "    Mean Precision: 0.43315\t\n",
    "    Mean Recall: 0.34500\n",
    "    \n",
    "The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.<br>\n",
    "In our project Precision is the ability of the classifier not to predict a person who is not a POI as a POI <br>\n",
    "The recall is intuitively the ability of the classifier to find all the positive samples.<br>\n",
    "In our project Recall is the ablity of the classifier to find all the POIs . <br>\n",
    "\n",
    "A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources :\n",
    "\n",
    "    http://scikit-learn.org/stable/modules/classes.htm\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
